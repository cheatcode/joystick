import c from"chalk";import l from"fs";import h from"os";import f from"../../lib/generate_id.js";import b from"../databases/get_target_database_connection.js";import m from"../databases/queries/map.js";import r from"../../lib/timestamps.js";import s from"../../lib/types.js";import p from"../../test/track_function_call.js";class g{constructor(t="",e={}){this.init_database=this.init_database.bind(this),this.machine_id=l.readFileSync(`${h.homedir()}/.cheatcode/MACHINE_ID`,"utf-8")?.trim().replace(/\n/g,""),this.name=t,this.options={concurrent_jobs:1,...e},this.init_database(this?.options?.external,this?.options?.database?.provider)}async init_database(t=!1,e=null){const n=e||b("queues")?.provider,o=m[n]?.queues,a=this._get_database_connection();s.is_object(a)&&s.is_object(o)&&(this.db=Object.entries(o||{})?.reduce((d={},[_,i])=>(d[_]=i.bind({db:a,machine_id:this.machine_id,queue:{name:this.name,options:this.options}}),d),{_connection:a}),t||(await this.db.initialize_database(n),(this?.options?.runOnStartup||this?.options?.run_on_startup)&&this.run()))}_get_database_connection(){if(this?.options?.database){const{provider:t,name:e}=this?.options?.database,n=process.databases&&process.databases[t]&&process.databases[t][e];return n||console.warn(c.red(`Connection to database ${t}.${e} not found on process. Cannot start queue.`)),n||null}return process.databases._queues}async add(t={}){const e=r.get_database_format(this?.db?._connection);let n;(t?.nextRunAt||t?.next_run_at)==="now"||!t?.nextRunAt&&!t?.next_run_at?n=r.get_future_time(null,0,{format:e}):n=r.normalize_date(t?.nextRunAt||t?.next_run_at,{format:e});const o={_id:f(16),status:"pending",environment:process.env.NODE_ENV,next_run_at:n,job:t?.job,payload:t?.payload},a=this?.options?.jobs&&this?.options?.jobs[t?.job];if(a&&(s.is_function(a?.preflight?.onBeforeAdd)||s.is_function(a?.preflight?.on_before_add))&&!await(a?.preflight?.onBeforeAdd||a?.preflight?.on_before_add)(o,this.db._connection,`queue_${this.name}`))return null;this.db.add_job(o)}async _check_if_okay_to_run_jobs(){return await this._get_number_of_jobs_running()<(this.options.concurrentJobs||this.options.concurrent_jobs||1)}_get_number_of_jobs_running(){return this.db.count_jobs("running")}_handle_requeue_jobs_running_before_restart(){if(this.db)return!this.options.retryJobsRunningBeforeRestart&&!this.options.retry_jobs_running_before_restart?this.db.delete_incomplete_jobs_for_machine():this.db.set_jobs_for_machine_pending()}run(){this.db&&(process.env.NODE_ENV!=="test"&&console.log(`Starting ${this.name} queue...`),this._handle_requeue_jobs_running_before_restart().then(()=>{setInterval(async()=>{if(await this._check_if_okay_to_run_jobs()&&!process.env.HALT_QUEUES){const e=await this.db.get_next_job_to_run();this.handle_next_job(e)}},300)}))}async handle_next_job(t={}){const e=this.options.jobs[t?.job];if(t&&t?.job&&e&&s.is_function(e?.run))try{if((s.is_function(e?.preflight?.okayToRun)||s.is_function(e?.preflight?.okay_to_run))&&!await(e?.preflight?.okayToRun||e?.preflight?.okay_to_run)(t?.payload,t))return this._handle_requeue_job(t,r.get_future_time("seconds",e?.preflight?.requeueDelayInSeconds||e?.preflight?.requeue_delay_in_seconds||10));if((s.is_number(e?.maxAttempts)||s.is_number(e?.max_attempts))&&t?.attempts>=parseInt(e?.maxAttempts||e?.max_attempts,10))return(s.is_function(e?.onMaxAttemptsExhausted)||s.is_function(e?.on_max_attempts_exhausted))&&await(e.onMaxAttemptsExhausted||e.on_max_attempts_exhausted)(t),this._handle_delete_job(t?._id);await this._log_attempt(t?._id),p(`node.queues.${this?.name}.jobs.${t?.job}`,[t?.payload]),await e.run(t?.payload,{...t,queue:this,completed:()=>this._handle_job_completed(t?._id),failed:n=>this._handle_job_failed(t,e,n),delete:()=>this._handle_delete_job(t?._id),requeue:(n="")=>this._handle_requeue_job(t,n)})}catch(n){console.warn(n),this._handle_job_failed(t,e,n)}}_log_attempt(t=""){return this.db.log_attempt(t)}_handle_job_completed(t=""){return this.db.set_job_completed(t)}_handle_job_failed(t={},e={},n=""){const o=r.get_database_format(this?.db?._connection);return e?.requeueOnFailure||e?.requeue_on_failure?this._handle_requeue_job(t,r.get_future_time("seconds",10,{format:o})):this.db.set_job_failed(t?._id,n)}_handle_delete_job(t=""){return this.db.delete_job(t)}_handle_requeue_job(t={},e=null){let n;return e?n=this._normalize_date_for_db(e):n=this._normalize_date_for_db(new Date),this.db.requeue_job(t?._id,n)}_normalize_date_for_db(t){const e=r.get_database_format(this?.db?._connection);return r.normalize_date(t,{format:e})}list(t=""){const e={};return t&&(e.status=t),this.db.get_jobs(e)}async normalize_job_dates(){if(!this.db)return{migrated:0,error:"No database connection available"};try{const t=r.get_database_format(this?.db?._connection)==="postgresql",e=await this.list();let n=0;for(const o of e){let a=!1;const d=["next_run_at","started_at","completed_at","failed_at"],_={};for(const i of d)if(o[i])try{t&&typeof o[i]!="string"?(_[i]=o[i].toISOString(),a=!0):!t&&typeof o[i]=="string"&&(_[i]=new Date(o[i]),a=!0)}catch(u){console.warn(`Could not normalize ${i} for job ${o._id}:`,u)}a&&(t?await this.db._connection.query(`
              UPDATE queue_${this.name}
              SET ${Object.keys(_).map((i,u)=>`${i} = $${u+2}`).join(", ")}
              WHERE _id = $1
            `,[o._id,...Object.values(_)]):await this.db._connection.collection(`queue_${this.name}`).updateOne({_id:o._id},{$set:_}),n++)}return{migrated:n,total:e.length}}catch(t){return console.error("Error normalizing job dates:",t),{migrated:0,error:t.message}}}}var O=g;export{O as default};
